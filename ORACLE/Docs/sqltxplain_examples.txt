=========================================================
                         ---SQLT---
===========================================================
GRANT SQLT_USER_ROLE to kh85124;
EXEC sqltxadmin.sqlt$a.set_sess_param('connect_identifier', '@PRD1'); 
disable TUNING and DIAG pack
SQL> EXEC sqltxadmin.sqlt$a.disable_tuning_pack_access; or SQL> EXEC sqltxadmin.sqlt$a.disable_diagnostic_pack_access; 
 
-------------
XTRACT Method
--------------
Use this method if you know the SQL_ID or the HASH_VALUE of the SQL to be analyzed, else use XECUTE. The SQL_ID can be found on an AWR report, and the HASH_VALUE on any SQL Trace (above the SQL text and identified by the "hv=" token).

# cd sqlt/run
# sqlplus apps
SQL> START sqltxtract.sql [SQL_ID]|[HASH_VALUE] [sqltxplain_password]
SQL> START sqltxtract.sql 0w6uydn50g8cx sqltxplain_password
SQL> START sqltxtract.sql 2524255098 sqltxplain_password
-------------


------------
XECUTE Method
------------
This method provides more detail than XTRACT. As the name XECUTE implies, it executes the SQL being analyzed, then it produces a set of diagnostics files. Its major drawback is that if the SQL being analyzed takes long to execute,
this method will also take long.


# cd sqlt
# sqlplus apps
SQL> START [path]sqltxecute.sql [path]scriptname [sqltxplain_password]
SQL> START run/sqltxecute.sql input/sample/script1.sql sqltxplain_password

------------
XTRXEC Method
------------
This method combines the features of XTRACT and XECUTE. Actually, XTRXEC executes both methods serially. The XTRACT phase generates a script that contains the extracted SQL together with the binds declaration and assignment for an 
expensive plan found for the requested SQL statement. XTRXEC then executes the XECUTE phase using the script created by the first.


# cd sqlt/run
# sqlplus apps
SQL> START sqltxtrxec.sql [SQL_ID]|[HASH_VALUE] [sqltxplain_password]
SQL> START sqltxtrxec.sql 0w6uydn50g8cx sqltxplain_password
SQL> START sqltxtrxec.sql 2524255098 sqltxplain_password

----------
XPLAIN Method
----------
This method is based on the EXPLAIN PLAN FOR command, therefore it is blind to bind variables referenced by your SQL statement. Use this method only if XTRACT or XECUTE are not possible.



 # cd sqlt
# sqlplus apps
SQL> START [path]sqltxplain.sql [path]filename [sqltxplain_password]
SQL> START run/sqltxplain.sql input/sample/sql1.sql sqltxplain_password
----------
XPREXT Method
----------
Use this method if you have used XTRACT and you need a faster execution of SQLT while disabling some SQLT features. Script sqlt/run/sqltcommon11.sql shows which features are disabled.


# cd sqlt/run
# sqlplus apps
SQL> START sqltxprext.sql [SQL_ID]|[HASH_VALUE] [sqltxplain_password]
SQL> START sqltxprext.sql 0w6uydn50g8cx sqltxplain_password
SQL> START sqltxprext.sql 2524255098 sqltxplain_password

--------
XPREXC Method

Use this method if you have used XECUTE and you need a faster execution of SQLT while disabling some SQLT features. Script sqlt/run/sqltcommon11.sql shows which features are disabled.

--------
# cd sqlt
# sqlplus apps
SQL> START [path]sqltxprexc.sql [path]scriptname [sqltxplain_password]
SQL> START run/sqltxprexc.sql input/sample/script1.sql sqltxplain_password

--------
COMPARE Method

Use this COMPARE method when you have two similar systems (SOURCES) and the same SQL statement performs fine in one of them but not in the other. 
-------
# cd sqlt
# sqlplus sqltxplain
SQL> START [path]sqltcompare.sql [STATEMENT_ID 1] [STATEMENT_ID 2]
SQL> START run/sqltcompare.sql 92263 72597
SQL> START run/sqltcompare.sql

---------
TRCANLZR Method

This method takes as input a SQL Trace filename and proceeds to analyze this file. The actual trace must be located in the TRCA$INPUT1 directory, which defaults to the USER_DUMP_DEST directory during installation.

---------
# cd sqlt
# sqlplus [application_user]
SQL> START [path]sqltrcanlzr.sql [SQL Trace filename|control.txt]
SQL> START run/sqltrcanlzr.sql V1122_ora_24292.trc
SQL> START run/sqltrcanlzr.sql control.txt
--------------
TRCAXTR Method

This method does the same than TRCANLZR but when the trace analysis completes, it continues with a XTRACT for the Top SQL found in the trace. Basically it consolidates all the generated reports by TRCANLZR and XTRACT on the Top SQL.
--------------

# cd sqlt/run
# sqlplus [application_user]
SQL> START sqltrcaxtr.sql [SQL Trace filename|control.txt]
SQL> START sqltrcaxtr.sql V1122_ora_24292.trc
SQL> START sqltrcaxtr.sql control.txt



'RECURSIVE CALL' 
reflect Oracle querying the data dictionary to get information about the objects (tables, indexes, privileges) 
in your query. If the current in-memory tables don't have any information about the query objects (maybe that table hasn't been queried before)
Oracle will have to do a lot of queries to get information about ALL of the objects referenced by your query.The second (and third) time you run
your query Oracle already has that data dictionary information for those objects and doesn't have to query it again so the count shows zero.

Here is a link to a blog by Jonathan Lewis that explains 'db block get' and 'consistent get'
http://jonathanlewis.wordpress.com/2009/06/12/consistent-gets-2/

 
=====================================================================================
ANALYSING SQLTEXPLAIN REPORT
=====================================================================================
 
 
1. “fluctuating number of histogram endpoints count” (Table contains columns referenced in predicates with fluctuating number of histogram endpoints count) : 

Every time we gather CBO statistics on a Table, we have a new “version” of the statistics. We store the old one automatically (DBMS_STATS does that starting on 10g) and the new version becomes “current”.
We store 31 days of history by default. So if we gather statistics on a Table lets say daily, we may have about 30 statistics versions for that Table and its columns (and Indexes). 
Now, if a column has lets say 5 buckets in its histogram and the next version has 6, there was a recorded change of 20%, which may be relevant for your investigation.
If a column during its stored history has a number of buckets that fluctuates more than 10% from one version to the next, then it is flagged as “fluctuating” (use to be mutating in SQLT 11.4.4.8 and older).
Changes of less than 10% are ignored.

Same concept applies to other elements like the number of distinct values in a column, or the height of an index, or the number of rows in a Table or Partition. 
If they fluctuate more than 10% from one version of the CBO statistics to the next, then they are flagged and reported in the Observations section of the main SQLT report.

ACTIONS : All depends. Lets say your SQL suddenly stop using index X and now uses index Y. 
          If blevel on X has increased recently, you can suspect a cost increased due to the new blevel and investigate accordingly. 
          Keep in mind that “Observations” section of SQLT, which reports outcome of health-checks, is basically a collection of heads-up.
          Things to investigate and maybe pointing to the root cause of a recent change in an execution plan for the SQL being investigated. 
 
 
 
 
 - Selectivity Predicate Selectivity:                            - EqualityPredicate Cardinality
 0.000209                                                          1 
                                                                 As the cardinality increase Selectivity also increase . 
Selectivity [expected rows] vary from 0 to 1                     Cardinality expected rows from 0 to total number of rows(excluding nulls).
                                                                 Higher cardinality = higher cost (outdated stats) 

 explanation: will get %0.0209 of the Table if there are matching rows or 0.000209 chance of having 1 row back .  
 

-CBO global stats (synthesized)
 Estimated SREADTIM( single block read time): always lower than MREADTIM -±9                   - Estimated MREADTIM (multiple block read time) : always higher than SRAEADTIM ±22


To fix SREADTIM>MREADTIM:
1- Choose workload time 
2- Create statistic table (SYSTEM_STATISTICS)
  exec DBMS_STATS.CREATE_STAT_TABLE('SYS','SYSTEM_STATISTICS');
3- run Gather_system_stats procedure during chosen workload
BEGIN 
DBMS_STATS.GATHER_SYSTEM_STATS('',interval=>120,stattab=>'SYSTEM_STATISTICS',statid=>'WORKLOAD');
END;
/

4- Import those statistics using DBMS_STAT.IMPORT_SYSTEM_STATS
 exec DBMS_STATS.IMPORT_SYSTEM_STATS(stattab=>'SYSTEM_STATISTICS',statid=>'WORKLOAD',statown=>'SYS');
 
 check the changed stat info :
 SQL>SELECT * FROM sys.aux_stats$;
 
 Delete Old statistics when not needed:
 SQL>exec DBMS_STATS.DELETE_SYSTEM_STATS;
 
  
 
 2. Plan performance history (AWR only):
   - Click Performacen History under Plans. check the Opt env Hash value to see if there was a change over Optimizer environment Vs plan Hash
 3. Column Statistics :
   * Table contain x columns in predicates where distinct values nbr unmatches the number of buckets.
   * Histograms :frequency => one bucket per distinct value (example bucket 1 , value :1 count 300) the higher count the least cost involved. 
   No good reason to have less than 255 (if a value in the midle but no bucket is assigned it'll be interpolated  
   
   - example A : customer_name : stelios =row count 100 ,Steven =110  . After a while Stephan is introduced with 500 records but will be placed between Buckets Stelios and steven assuming 105 records. 
         fix : run the gather_table_stats  
  
  * Out-of-range Values : if a value is out of range Optimizer should guess and give cardinality of 1 by default.
  fix : Run xceute go to predicate> click more> view the predicates > 
                                                check what is wrong on column statistics related to the prdicates :
                                                a- missing bucket      b- No histogram but highly skewed data                
  * Over Estimates and UnderEstimates : In the execution plan go the top right corner Last Over/Under estimate
         - If Estim_card * starts < output_rows then under-estimate.  10x Under 
         - If estim_card * starts > output_rows then over-estimate.   72x over  (>10 is highlighted in red)
         fix: click more uner each line having big under over/estimate values and check why there is an estimation error (stale stats,skewed data>histogram
         ..etc).
chap 2-Mysterious Change : Common checks when a plan if new indexes been added ,stats ran,Hints,Sys stats,CBO parameter (Optimizer_Index_cost=100 instead of 1)         
  
chap 3 statistics :
COST of every  operation is made up :
1- cost of single block reads  (inde xScan)
2- Cost of multi-Block reads  (FULL Scan)
3- Cost of the CPU used to do everything

    INDEX SCAN Cost- 1000 means 1000 single block reads x 12ms(single block read time) =12 second 
    FULL TABLE Cost - based on number of rowslength of the ows speed of disk 
    * New Prtitions need to be analysed after creation if not use SQL profiles 
    * Stale percentage can be changed using : exec dbms_stats.Set_table_prefs(null,'SCHEMA','STALE_PERCENT',6) ;
    * SAMPLE SIZE : in Column Statistics If random samples don't pick rare skewed values and if these values are popular in your queries ... You should adjust the SAMPLE size.  
         Col Pos In Pred In Proj Col ID	Column Name   Descend	Num Rows Num Nulls  Sample Size	Perc	Num Distinct	
        ------- -------- ------- ------ ----------- ---------- --------- ---------- ----------- ------ --------------- -
         1	 [+]	  TRUE	    9	 DATE_DEBUT	ASC	7660388	  0	     9175	 0.1	6858		
   * You can also lock statistics when they are very volatile  :
   DBMS_stats.LOCK_TABLE_STATS(ownname=>'SCHEMA',tbname=>'TEST2')
   * import a statistic for a later use (as snapshot for a table) :
   SQL> exec dbms_stats.create_state_table('mySCHEMA','MYstatS2'); --- The stat table is called MyStats2 
        Save TEST2 table statistic for a later used
  SQL>  exec dbms_stats.import_table_stats('myschema','TEST2',null,'MYSTATS2'); --- save it to the archive stat table    and use it even in a test environment without requiring data
  
  * statistics bad timing calculation use case :
   operation ID has Estimated cardinalité of 1 ,last output rows 1 millions with 0 as cost
   >> click +more link col statistics under "index columns"  > check num rows =0    
      click +more link col statistics under "Table columns" > check num =0 and last analyzed is = 4am 
      back to table section click table modification > 2 milion rows added around 5:06pm same day     	
    SOLUTION : - collect statistics at a different time - export import statistics as appropriate    - Freeze the statsistics for when the table is populated (check also if parameters like _optimizer_cartesian_enabled=FALSE impact the join used in the exec plan.) 
=====================
CHAPTER  4: Skewness
=====================
   When data is highly skewed and autosampling is tiny (38 distinct value in 5 milion rows) Choosing a larger sample might be a good idea. 
 
   How to find skewness example :
    SQL> create table test3(object_type) as select object_type from dba_objects;
    SQL> select object_type , count(object_type) from TEST3 group by object_type order by count(object_type);
            OBJECT_TYPE             COUNT(OBJECT_TYPE)
        ----------------------- ------------------
        SYNONYM                              37132
        RESOURCE PLAN                           11
    
     - If some predicates return more than 10 times the vales of other predicates or one bucket having 90% of column values then data is highly skewed.
       10% varibility between bucket causes no skewness problem .
     - Check Tables/Columns/histograms section in SQLT:       
       Tables > table Cols or Histograms : click on frequency link at TEST3 ; object_type column
        test3.OBJECT_TYPE : VALUE 'DIEMNSION' has low cardinality :13 which implies low selectivity ± 0.000182 
                         VALUE 'JAVA CLASS' has cardinality of 23083 and selectivity of 0.313704  which means the the 3rd of the table will be returned if 'JAVA CLASS' was selected in the the predicate. 
                         if not involved in a predicat skewness doesn't affect the query
                         
    Example : Financial transction table based on company IDs: if ID has only few records => cartesian Join might be used 
                                                                if ID (big bank) milions of records implies a Hash join .
        CAUSES   : Poor sampling histograms,inappropriate histograms,missings histograms.                                                               
        SOLUTION : for multiple exec plans due to skewnes:1-remove Histogram when not needed ,2- improving histograms when poor, use adaptive cusror sharing ,adding histograms when needed, change the query
      HISTOGRAMS : Sometimes statistically sampled data won't get all posible values of the column
             - Hence even if it is a frequency -Histogram (less than 254 bucket) not all values are represented ( 33 buckets/5k sample size for 45 distinct values)                                                                                
             - If we squeeze to 10 buckets histogram becomes a Height based histogram : 
             SQL> exec dbms_stats.set_table_perfs(ownname=>'schema',tabname=>'TEST3',pname=>'methode_opt',pvalue=>'for all columns size 10');
             height based histogram has values are marked as popular or not popular . if not popular one bucket is allocated exemple :            
         BUCKET 1 : Estimated endpoint value "CONSUM" including all values named lower in alpabet than CONSUM (CLUSTEr,CONSUMER,GROUP'
         BUCKET 4 : SYNONY1 encapsulate 4 previous buckets and being just a generated label (value is truncated)
         CONCLUSION :  only 10 buckets for 45 DNV = poor quality histogram = poor execution plan .
        
        Sample  Perc	NumDistinct Low Value2     High Value2	 Last Analyzed	     Avg Col Len  Density	Num Buckets  Histogram
        ------- ------ ----------- -------------- ------------- -------------------- ----------- ------------- ------------ ------------
         9175	0.1	2577	      "-2"	  "1302484107"   2018-02-21/22:29:37	6	  1.670000	254	     HYBRID      ---low percentage
         45296  17.0	76	" 2009/01/01"	  "2018/02/01"	 2018-02-21/22:29:37	6	  3.84    	75           FREQUENCY 
      Observation : some samples % are too low (0.1) and 1 histogram has 17% sample and 75 bucket for 76 distinct values  (one value notrepresented in the sample). If selectivity varies from 0.005 to 0.1 factor is 20 = skewed Column data   
       
    Commands : 
      ADD histogram : SQL> exec dbms_stats.delete_column_stats (ownname=>'schema',tbname=>'TEST3',colname => 'OBEJCT_TYPE',stat_type=>'HISTOGRAM'); 
      REMOVE all histograms   SQL>exec dbms_stats.set_table_prefs(ownname=>'schema',tbname=>'TEST3',pname=>'method_opt',pvalue=>'FOR ALL COLUMN SIZE 1');  
      REMOVE object_type column Histogram SQL> exec  dbms_stats.set_table_prefs(ownname=>'schema',tbname=>'TEST3','pname=>'method_opt',pvalue=>'FOR OBJECT_TYPE COLUMN SIZE 1');       
      Gather column stats : SQL> exec dbms_stats.get_column_stats (ownname=>'schema',tbname=>'TEST3',colname => 'OBEJCT_TYPE');                   
  
  
     BIND PEEKING/CAPTURE :
        Bind peeking: is access to the value of the BIND variable during hard parse .   
        Bind capture : if the binds are pushing Optimizer to run a poor execution plan . check SQLT bind capture to track the culprit statement.  
        Cursor sharing :similar converts all similar queries with absolute values into one cursor -- deprecated in 11g.
    Executions Plans variation on xecute: [W]:worst exec plan [B] Best. check Lat/Over estimate and rows returned 

     ID	Exec Ord	OperationGo To	                                                        More Peek Bind	Capt Bind	Cost2	Estim Card	LASTStarts LASToutput Rows	LAST Over/Under Estimate1	LAST  CR Buffer Gets2
         2	....+....+....+....+.... TABLE ACCESS BY INDEX ROWID BATCHED R_CRITERERECHERCHE	[+]	[+]	 	 	 4	2	         1	      19	        10x under	                62	3	0.051	 
     26	 1	....+....+....+....+....+ INDEX RANGE SCAN R_CRITERERECHERCHE_IDX_1	[+]	[+]	[+]	[+]	         3	1	         1	      72	        * 72x under	                3	1	0.017	 
================================================   
CHAPTER  5: TROUBELSHOOTING QUERY TRANSFORMATION   
================================================

    Hints : 
    /*+ no_query_transformation */ to avoid transformation. 
    /*+ first_rows(n) , unnest/no_unnest, push_pred/no_push_pred, push_subq/No_push_subq ,native_full_outer_join, no_set_to_join,qb_name
    types: 
    * Sub-query unnesting (VIEW-VIEW-VIEW operation for each subquery call)
    * Complex view Merging: CVM
    * Joi predicate Push down :PPD 
 
10053 Trace File: more detailed information on what decision and steps optimizer made during parsing . 
      The file is found within the SQLT report folder : nsqlt_sxxxx_xxxx_xxxxx_execute.trc
         Or manually generated : 
         SQL> ALTER SESSION SET MAX_DUMP_FILE_SIZE=UNLIMITED;
         SQL>ALTER SESSION TRACEFILE_IDENTIFIER='MY_10053_TRACE';
         SQL>ALTER SESSION SET EVENTS '10003 TRACE NAME CONTEXT FOREVER, LEVEL 1';
         SQL>Select sysdate from dual;
 located in diag/rdbms/oracle_sid/
      trace filing query inside a PL/SQL Block : 
      SQL> alter session set tracefile_identifier='PLSQL'; 
      SQL> alter session set events 'trace[rdbms.SQL_OPtimizer.*][sql:9rjmrbhjuasavPLSQLBLOCK_SQLID]' ;
      run BLOCK 
      turn off tracing: SQL> alter session set events 'trace[rdbms.SQL_Optimizer.*]off'; 
 
 
 DBMS_SQLDIAG : Allows you to only grab 10053 related info providing the related SQL_ID
    exemple :
   SQL>select sysdate from dual; 
    SQL> select sql_id from v$sql where sql_text like 'select sysdate from dual%';
       SQL_ID
       -------------
       7h35uxf5uhmm1
    SQL> exec dbms_sqldiag.dump_trace(p_sql_id=>'7h35uxf5uhmm1',p_child_number=>0,p_file_id=>'DIAG');
    SQL> host dir C:\APP\ORACLE\PRODUCT\12.1.0.2\DB_1\RDBMS\TRACE\*DIAG*.trc  to get the file 
            
            
  READING 10053 TRACEFILE:
   Sections:
  A- OPTIMIZER PARAMETERS:  Any changed parameter before the tracing would be listed in the Altered values sections.
   - PARAMETER WITH ALETERED VALUES    - PARAMTER WITH DEFAULT VALUES (optimizer_mode_hinted ,..*)  
  
  B- OPTIMIZER HINTS: 
  After using hints on a query and tracing it. We can find the related information in the 10053 trace FILE under :
  Dumping Hints section: used=0 means hint not used . err=4 means there is conflict.
  =====================
  atom_hint=(err=0 resol=1 used=0* token org lvl txt=USE_MERGE("B")) ,USE_NL ...etc    
  C- COST CALCULATION : 
  after 10053 traceshowing basic information and considering SQL query and possible query transformation , CBO runs and displays basic access cost related info. 
   
   BASE STATISTICAL INFOMATION
  *******************************       
   Table Stats : 
   Index Stats : CLUF : clustering factor 
   SINGLE TABLE ACESS PATH  
   table (EMPLOYEES)
   acess path : Table scan    Cost :3
   access Path : index (AllEqrange) Cost : 2
   BEST:: Acces Path :indexRange index: EMP_DEPARTMENT_IX
   
   Once this is done Optimizer will consider join orders 
   plan cardinality msmatch: best card=6.55
   join order aborted : cost > best plan cost .
   
================================================   
CHAPTER  7: ADAPTIVE CUSROR SHARING   
================================================   
   
  - Seek "Cursor sharing and Binds" section. 
   BIND sensitive : Optimizer suspects that some BIND variables may affect execution plan but not shure 
   BIND aware : BIND variables really make significant difference to number of buffer gets ±
  - Setting BIND Sentitive Cursor:
  Tasks: Create table populate it with skewed data, select popular value, moderately popular then popular value and finaly a rare value  number of times .
   check sqlTXECUTE and sqltXTRACT report for results : 
   
   EVERY PLAN linked to BIND variable based on Histogram is BIND SENSITIVE.
   Becomes BIND aware after few execution based on rare value (Bucket less than 1K).Idea is not to create a bucket if a value is rare enough but keep a separate plan for each Bind value so it can chose matching Plan during BIND peeking 
   
 DISABLE ACS : 
 SQL> Alter system set "_optimizer_extended_cursor_sharing_rel"=none scope=both; 
 SQL> Alter system set "_optimizer_extended_cursor_sharing"=none scope=both;    
     
================================================   
CHAPTER  8: DYNAMIC SAMPLING   
================================================                

Abreviation : (DS) Dynamic sampling     - (CFB) Cardinality Feed Back  
             
Dynamic sampling steps :
 
 1- Optimizer starts parsing the query
 2- During parsing Optimizer checks objects stats state            
 3- If some stats missing, DYNAMIC SAMPLING execution and the sampling amount depends on optimizer_dynamic_sampling variable(default 2)
 4- If Dynamic sampling is done the statistics gathered are used to genereate a better plan 
 5- If no Dynamic sampling to be used rest of Optimzer process continues and stats used where available   
          
          
CONTROL DYNAMIC SAMPLING :
    Session level : Alter session set optimizer_dynamic_sampling=4;
    System level  : Alter system set optimizer_dynamic_sampling=4 scope=(spfile,memory);
    Hint cursor level : SELECT /*+ dynamic_sampling (4) */ from dba_objects;     Hint table level: select /*+ ynamic_sampling(dba_objects 4) from dba_objects; 
 - Values of dynamic sampling (0-10) based on the number of block s to sanmple at the last moment before execution .
   0 : No DS used
   1 : If at least 1 unalyzed,unindexed,non partitioned table and table >32blocks , then 32blocks are sampled but table is indexed or <32blocks no DS is done
   2 : At least 1 table (indexed or not) with no statistics,64blocks are sampled
   3 : Level 2 + there is where clause with an expresssion 64 blocks
   4 : Level 2  + there is OR or AND operator on predicate 64 blocks 
   5 : Level 4, 128 blocks 
   6 : Level 4,256 blks    7 : Level 4 512 blks   8 : level 4 1024 blks  9 : level 4 ,4086         
 
 - Check Dynamic sampling process : 
   Start 10053 trace and some query then consult the file :
   / Generated dynamic sampling query :    
     query text : SELECT /* OPT_DYN_SAMP */ /*+ALL_ROWS IGNORE_WHERE_CLAUSE NO_PARALLEL(SAMPLESUB) opt_param('parallel_execution_enables','false') NO_PARALLEL_INDEX(SAMPLESUB) NO_SQL_TUNE */
     NVL(SUM(C1),0), NVL(SUM(C2),0) FROM (SELECT /*+NO_PARALLEL("SALES") FULL("SALES") NO_APARALLEL_INDEX("SALES") */ 1 AS C1,1 AS C2 FROM "SALES" SAMPLE BLOCK(2.035,1) SEED (1) "SALES")
     SAMPLESUB 
  Executed dynamic sampling ,level:4 sample percent:2.035 , total partitions :28 actual sample size :18860<<
  Original estimate cardinality: 145.484 , *** Dynamic sampling cardinality : 926.759 ,actual cardinality :918843, block cnt : 36-64 
          
 - HOW TO FIND OPTIMIZER_DYNAMIC_SAMPLING value:         
  > CBO Environment> Optimizer_Dynamic_Dampling  or SQL> show parameter optimizer_dynamic_sampling       
  
  - PARALLELISME : if query passes through parallelisme and DS is at 2 then 64 block sampling is ignored and sampling depends on table size and predicate complexity .
  -  IF statistics exist on the table but DS is still executed then it may be bceause of the predicate complexity or lack of extended statistics .     
  - DISABLE : set optimizer_dynamic_sampling parameter to 0
  - Cases : global temporary table or all tables that are being populated during the query execution.        
          
CARDINALITY FEEDBACK:
PARAMETER : "_optimizer_use_feedback" HINT /*+ opt_paqram('_optimizer_use_feedback','false')*/
 Wait for results of esach step of the execution plan , store in the shared pool and reference it on subsequent executions (to assess the plan efficiency).
 *              
          
Parar, Value
SQL_ID, bxsdc55bbtkcp
SHARABLE_MEM, 132857
PERSISTENT_MEM, 135760
RUNTIME_MEM, 133848
SORTS, 0
LOADED_VERSIONS,1
OPEN_VERSIONS,1
USERS_OPENING,1
FETCHES,0
EXECUTIONS,1
PX_SERVERS_EXECUTIONS,0
END_OF_FETCH_COUNT,0
USERS_EXECUTING,1
LOADS,1
FIRST_LOAD_TIME,2014-01-02/10:00:22
INVALIDATIONS,0
PARSE_CALLS,1
DISK_READS,0
DIRECT_WRITES,0
BUFFER_GETS,208714
APPLICATION_WAIT_TIME,0
CONCURRENCY_WAIT_TIME,938965
CLUSTER_WAIT_TIME,0
USER_IO_WAIT_TIME,0
PLSQL_EXEC_TIME,0
JAVA_EXEC_TIME,0
ROWS_PROCESSED,0
COMMAND_TYPE,3
OPTIMIZER_MODE,ALL_ROWS
OPTIMIZER_COST,402603
OPTIMIZER_ENV_HASH_VALUE,3694337849
PARSING_USER_ID,140
PARSING_SCHEMA_ID,140
PARSING_SCHEMA_NAME,HF53ES
KEPT_VERSIONS,0
ADDRESS,070000100E6CA970
TYPE_CHK_HEAP,00
HASH_VALUE,1455212949
OLD_HASH_VALUE,2884697757
PLAN_HASH_VALUE,2120743520
CHILD_NUMBER,0
SERVICE,SYS$USERS
SERVICE_HASH,0
MODULE,SQL*Plus
MODULE_HASH,-625018272
ACTION,
ACTION_HASH,0
SERIALIZABLE_ABORTS,0
OUTLINE_CATEGORY,
CPU_TIME,20980000
ELAPSED_TIME,28541037
OUTLINE_SID,
CHILD_ADDRESS,070000100ECBC3D8
SQLTYPE,6
REMOTE,N
OBJECT_STATUS,VALID
LITERAL_HASH_VALUE,0
LAST_LOAD_TIME,2014-01-02/10:00:22
IS_OBSOLETE,N
CHILD_LATCH,7
SQL_PROFILE,
PROGRAM_ID,0
PROGRAM_LINE#,0
EXACT_MATCHING_SIGNATURE,3,65659072652807E18
FORCE_MATCHING_SIGNATURE,1,82431115016361E19
LAST_ACTIVE_TIME,2/01/2014 10:00:52
TYPECHECK_MEM,0
