 extract ddl exp/imp:

select initial_extent,next_extent,index_name from dba_indexes where owner='GRH1' and initial_extent/1024/1024 >50;


 export table partitions: |
---------Parfile
USERID="/ as sysdba"
DIRECTORY=STAGING_DIR
DUMPFILE=expdp_.dmp
LOGFILE=expdp_FMBON_ODS0315.log
TABLES=ACG.FMBON_ODS_EVT_SALDO:P20130301
 ,ACG.FMBON_ODS_INV:P20130301
 ,ACG.FMBON_ODS_ISIN_REF:P20130301
 ,ACG.FMBON_ODS_INV_CTRDSTA:P20130301
 ,ACG.FMBON_ODS_INV_AGG1:P20130301
 ,ACG.FMBON_ODS_INV_INF:P20130301
 ,ACG.FMREP_ODS_EVT_SALDO:P20130301
 ,ACG.FMREP_ODS_INV_INF:P20130301
 ,ACG.FMREP_ODS_INV:P20130301
FLASHBACK_SCN=7504347013500
CONTENT=DATA_ONLY

For more info :
----------------
impdp help=y 
expdp help=y

 
Example 2: - Data-Only Unload of Selected Tables and Rows
- Perform a data-only unload of all tables in the human resources (hr) schema except for the tables countries and regions. 
- Rows in the employees table are unloaded that have a department_id other than 50.

DIRECTORY=dpump_dir1
DUMPFILE=dataonly.dmp
CONTENT=DATA_ONLY
EXCLUDE=TABLE:"IN ('COUNTRIES', 'REGIONS')"
QUERY=employees:"WHERE department_id !=50 ORDER BY employee_id"



REMAP to replace a schema at the import:
------------------------------------------
REMAP_SCHEMA=source_schema:target_schema
REMAP_DATAFILE=source_datafile:target_datafile
REMAP_TABLESPACE=source_tablespace:target_tablespace
transform=oid:n

execution:
expdp myuser/pass@DB parfile=t.par


-------- parallelism Parfile
 
DUMPFILE=expdp.common.SGBDMA.13042011_%U.dmp   ----%U to split the dumps according to the // degree
LOGFILE=expdp_common.log
DIRECTORY=DATA_PUMP_DIR
CONTENT=ALL
TABLES=COMMON.D_ENT,COMMON.D_APP,COMMON.R_CS,COMMON.D_CTYI,COMMON.D_SEC,COMMON.D_NACE,COMMON.D_CRYI
FLASHBACK_SCN=7496159147375     ---- specific consistent  SCN 
parallel=16

--------------parfile
DUMPFILE=expdp.acg.SGBDMA.14042011_%U.dmp
LOGFILE=expdp_acg.log
DIRECTORY=DATA_PUMP_DIR
CONTENT=ALL
TABLES=ACG.D_AMT,ACG.GL_D_SCHMB,ACG.GL_D_BSTR,ACG.D_MISSEG_MI,ACG.GL_D_VAT,ACG.GL_D_SCHMC,ACG.GL_D_MACODE,ACG.D_EVT,ACG.GL_D_SCHMA,ACG.GL_R_ACGL,ACG.D_PRD_MI
FLASHBACK_SCN=7513619723024
parallel=16


------------execution
expdp FF76FK DUMPFILE=expdp.D_CUSTREF_MI.SGBDMA.14042011_%U.dmp LOGFILE=expdp_D_CUSTREF_MI.log DIRECTORY=DIR_TEMP_DUMP TABLES=ACG.D_CUSTREF_MI parallel=16 CONTENT=ALL FLASHBACK_SCN=7470762218681

------------parfile
DUMPFILE=expdp.GL_F_ACEN.SGBDMA.15042011.dmp
LOGFILE=expdp_GL_F_ACEN.log
DIRECTORY=DIR_TEMP_DUMP
CONTENT=DATA_ONLY
TABLES=ACG.GL_F_ACEN:P20110101,ACG.GL_F_ACEN:P20110102,ACG.GL_F_ACEN:P20110103,ACG.GL_F_ACEN:P20110104,ACG.GL_F_ACEN:P20110105,ACG.GL_F_ACEN:P20110106,ACG.GL_F_ACEN:P20110107,ACG.GL_F_ACEN:P20110108,ACG.GL_F_ACEN:P20110109,ACG.GL_F_ACEN:P20110110,ACG.GL_F_ACEN:P20110111,ACG.GL_F_ACEN:P20110112,ACG.GL_F_ACEN:P20110113,ACG.GL_F_ACEN:P20110114,ACG.GL_F_ACEN:P20110115,ACG.GL_F_ACEN:P20110116,ACG.GL_F_ACEN:P20110117,ACG.GL_F_ACEN:P20110118,ACG.GL_F_ACEN:P20110119,ACG.GL_F_ACEN:P20110120,ACG.GL_F_ACEN:P20110121,ACG.GL_F_ACEN:P20110122,ACG.GL_F_ACEN:P20110123,ACG.GL_F_ACEN:P20110124,ACG.GL_F_ACEN:P20110125,ACG.GL_F_ACEN:P20110126,ACG.GL_F_ACEN:P20110127,ACG.GL_F_ACEN:P20110128,ACG.GL_F_ACEN:P20110129,ACG.GL_F_ACEN:P20110130,ACG.GL_F_ACEN:P20110131,ACG.GL_F_ACEN:P20110201,ACG.GL_F_ACEN:P20110202,ACG.GL_F_ACEN:P20110203,ACG.GL_F_ACEN:P20110204,ACG.GL_F_ACEN:P20110205,ACG.GL_F_ACEN:P20110206,ACG.GL_F_ACEN:P20110207,ACG.GL_F_ACEN:P20110208,ACG.GL_F_ACEN:P20110209,ACG.GL_F_ACEN:P20110210,ACG.GL_F_ACEN:P20110211,ACG.GL_F_ACEN:P20110212,ACG.GL_F_ACEN:P20110213,ACG.GL_F_ACEN:P20110214,ACG.GL_F_ACEN:P20110215,ACG.GL_F_ACEN:P20110216,ACG.GL_F_ACEN:P20110217,ACG.GL_F_ACEN:P20110218,ACG.GL_F_ACEN:P20110219,ACG.GL_F_ACEN:P20110220,ACG.GL_F_ACEN:P20110221,ACG.GL_F_ACEN:P20110222,ACG.GL_F_ACEN:P20110223,ACG.GL_F_ACEN:P20110224,ACG.GL_F_ACEN:P20110225,ACG.GL_F_ACEN:P20110226,ACG.GL_F_ACEN:P20110227,ACG.GL_F_ACEN:P20110228,ACG.GL_F_ACEN:P20110301,ACG.GL_F_ACEN:P20110302,ACG.GL_F_ACEN:P20110303,ACG.GL_F_ACEN:P20110304,ACG.GL_F_ACEN:P20110305,ACG.GL_F_ACEN:P20110306,ACG.GL_F_ACEN:P20110307,ACG.GL_F_ACEN:P20110308,ACG.GL_F_ACEN:P20110309,ACG.GL_F_ACEN:P20110310,ACG.GL_F_ACEN:P20110311,ACG.GL_F_ACEN:P20110312,ACG.GL_F_ACEN:P20110313,ACG.GL_F_ACEN:P20110314,ACG.GL_F_ACEN:P20110315,ACG.GL_F_ACEN:P20110316,ACG.GL_F_ACEN:P20110317,ACG.GL_F_ACEN:P20110318,ACG.GL_F_ACEN:P20110319,ACG.GL_F_ACEN:P20110320,ACG.GL_F_ACEN:P20110321,ACG.GL_F_ACEN:P20110322,ACG.GL_F_ACEN:P20110323,ACG.GL_F_ACEN:P20110324,ACG.GL_F_ACEN:P20110325,ACG.GL_F_ACEN:P20110326,ACG.GL_F_ACEN:P20110327,ACG.GL_F_ACEN:P20110328,ACG.GL_F_ACEN:P20110329,ACG.GL_F_ACEN:P20110330,ACG.GL_F_ACEN:P20110331
FLASHBACK_SCN=7470805188761
parallel=16
ESTIMATE_ONLY=YES
ESTIMATE=STATISTICS


-------------------------------------------------------------------------------------------------------
import: |    GRANT ON DIRECTORY    >>>   Grant read, write on directory some_dir to user1;
---------
impdp directory=expdir dumpfile=myexp.dmp sqlfile=ddl.sql
impdp FF76FK parfile=t.par
------------------------------







TO see IMPDP STATUS
---------------------

select * from dba_datapump_jobs where owner_name ='HF53ES';
OWNER_NAME                     JOB_NAME                       OPERATION
------------------------------ ------------------------------ ------------
HF53ES                         SYS_EXPORT_SCHEMA_01            EXPORT

 >>impdp / ATTACH=SYS_IMPORT_SCHEMA_01 
Import> status
 START_JOB, STOP_JOB, EXIT_CLIENT,KILL_JOB

Job: SYS_IMPORT_SCHEMA_01
  Operation: IMPORT                         
  Mode: SCHEMA                         
  State: EXECUTING                      
  Bytes Processed: 23,344,560,296
  Percent Done: 94
  Current Parallelism: 16
  Job Error Count: 0
  Dump File: /APPL/ORACLE/staging/expdp.FULL.SGCLOA.111130.2000.27437_%u.dmp
  Dump File: /APPL/ORACLE/staging/expdp.FULL.SGCLOA.111130.2000.27437_01.dmp
  ///////////////////////////////////////////////////////////////////////////
-----------------------------------------------------------------------------
exec DBMS_DATAPUMP.STOP_JOB (DBMS_DATAPUMP.ATTACH(‘SYS_EXPORT_SCHEMA_03',’SYSTEM’),1,0);

select sid,serial#,sofar,totalwork,start_time,
sysdate,time_remaining,message
from v$session_longops 
where opname like '%PORT%';


----expdp Parfile with query
DIRECTORY=DIR_TEMP_DUMP
DUMPFILE=expdp.dwh.SGDWHA.29082011_%U.dmp
LOGFILE=impdp_dwh_D_PRS.log
TABLES=DWH.D_PRS
QUERY= DWH.D_PRS:"WHERE TO_DATE('20110601','YYYYMMDD') BETWEEN DB_PRS AND DE_PRS"
parallel=16
CONTENT=DATA_ONLY
		where PARAGRAPHID in  (100
		,221
		,223
		,224
		,225
		,226
		,227
		,228
		,386
		,387
		,388)

		
impdp system/pwd dull=y directory=YOUR_DUMP_DIR exclude=SCHEMA:"='schema1,schema2'"
 
-----------WHEN TABLE EXISTS OPTIONS

HOST impdp system/xxxx@noida directory=dpump dumpfile=NEER_TEST.dmp table_exists_action=append
SKIP:     Default value for this parameter is SKIP. This parameter is exactly same as the IGNORE=Y option in conventional import utility.
APPEND:   This option appends the data from the data dump. The extra rows in the dump will be appended to the table and the existing data remains unchanged.
TRUNCATE: This option truncate the exiting rows in the table and insert the rows from the dump
REPLACE:  This option drop the current table and create the table as it is in the dump file. Both SKIP and REPLACE options are not valid if you set the  CONTENT=DATA_ONLY for the impdp


----expdp parfile
DIRECTORY=DIR_TEMP_DUMP
DUMPFILE=expdp.acg.SGBDMA.14042011_%U.dmp
LOGFILE=impdp_acg.log
TABLES=ACG.D_AMT,ACG.GL_D_SCHMB,ACG.GL_D_BSTR,ACG.D_MISSEG_MI,ACG.GL_D_VAT,ACG.GL_D_SCHMC,ACG.GL_D_MACODE,ACG.D_EVT,ACG.GL_D_SCHMA,ACG.GL_R_ACGL,ACG.D_PRD_MI
parallel=16
CONTENT=DATA_ONLY

----expdp parfile
DIRECTORY=DIR_TEMP_DUMP
DUMPFILE=expdp.common.SGBDMA.13042011_%U.dmp
LOGFILE=impdp_common.log
TABLES=COMMON.D_ENT,COMMON.D_APP,COMMON.R_CS,COMMON.D_CTYI,COMMON.D_SEC,COMMON.D_NACE,COMMON.D_CRYI
parallel=16
CONTENT=DATA_ONLY

impdp FF76FK DUMPFILE=expdp.D_CUSTREF_MI.SGBDMA.14042011_%U.dmp LOGFILE=impdp_D_CUSTREF_MI.log DIRECTORY=DIR_TEMP_DUMP TABLES=ACG.D_CUSTREF_MI parallel=16 CONTENT=DATA_ONLY

-------------------------------------------------------------------------------------------------------
IMPORT FROM MULTIPLE LOCATIONS
---------------------------------------------------
    
FLASHBACK_SCN= (select current_scn from v$database;)

alter table acg.AFT_OPS disable constraint AFT_OPS_CSTRBOK_R_CS_FK
alter table MART_PVA.PVA_ACEN_ESSAIBIS disable constraint FK1_CCTYGBK_D_CTYI

scp *.dmp oracle@sbeasajh:/ING/ORACLE/u14/SBDWHA/EB (push)
scp oracle@sbepsaeb:/ING/ORACLE/u59/SGDWHA/EB/*.dmp . (pull)

---IMPORT parfile

DUMPFILE=TEMP_DIR:expdp_ms9_tables_280212_part3_%U.dmp,DATA_STAGING_DIR:expdp_ms9_tables_280212_part3_%U.dmp
LOGFILE=imdp_ms9_tables_28_02_12_part3.log
CONTENT=DATA_ONLY
PARALLEL=16
TABLES=STG_MDPIDS1.MS9_REST_CD       
TABLES=STG_MDPIDS1.MS9_AR_ALT_IDENT  
TABLES=STG_MDPIDS1.MS9_REST_GRP_REL 
EXCLUDE=TABLE:"IN (NRA_FIX_STEP2','NRA_FIX','FIX_AU_BAL','BC_INTCV_AR_CD_STEP_2','BC_INTCV_AR_CD','BC054_ADR_INCOM_F')" 


----------------------------
EXPORT TO MULTIPLE LOCATIONS
----------------------------
expdp USER/******* dumpfile=expdir:filepart%u.dmp,reorg:filepart%u.dmp parallel=2 tables=hits_in logfile=reorg:logfile.exp
 
 
 
 ==================================================INTERNET SOURCE 
 Useful EXPDP Commands
Here you can find some useful EXPDP commands:

1) Export FULL database:

    expdp system/manager dumpfile=full.dmp directory=DATA_PUMP_DIR full=y logfile=full.log
 
2) Export database with multiple dump files:

     In some cases where the Database is in Terabytes and since the dump file size will be larger than the operating system limit, and hence export will fail. In those situations you can create multiple dump files by typing the following command.

     expdp  system/manager FULL=Y DIRECTORY=DATA_PUMP_DIR DUMPFILE=full%U.dmp FILESIZE=5G  LOGFILE=myfullexp.log JOB_NAME=myfullJob
    
     Note: This will create multiple dump files named full01.dmp, full02.dmp, full03.dmp and so on. The FILESIZE parameter specifies how large the dump file should be.

3) Export schema:

     Below command will be useful for taking backup of single schema (SCOTT) with using SCOTT credentials:

     expdp scott/tiger directory=DATA_PUMP_DIR dumpfile=scottexp.dmp logfile=scottexp.log

4) Export multiple schemas:

     Below command for taking backup of multiple schemas.

     expdp system/manager dumpfile=scott_demo.dmp directory=DATA_PUMP_DIR schemas=(scott,demo) logfile=scottexp.log
 
5) Export Table: 

     Below command for taking backup of single table (emp) under scott schema

     expdp scott/tiger dumpfile=scott_emp.dmp directory=DATA_PUMP_DIR tables=(emp) logfile=scottexp.log
 
6) Export multiple tables:

    Below command for taking backup of multiple tables (emp,dept) under scott schema
 
    expdp scott/tiger dumpfile=scott_emp_dept.dmp directory=DATA_PUMP_DIR tables=(emp,dept) logfile=scottexp.log
 
7) Export Tablespaces:

  Below command for taking backup of tablespace (users)
 
    expdp system/manager dumpfile=users_ts.dmp directory=DATA_PUMP_DIR tablespaces=(users) logfile=users_ts.log
 
8) Export multiple tablespaces:

     Below command for taking backup of multiple tablespaces (users, hr)

    expdp system/manager dumpfile=users_ts.dmp directory=DATA_PUMP_DIR tablespaces=(users, hr) logfile=users_ts.log
 
9) Export table with data only:

     Below command for taking single table with data only
 
     expdp scott/tiger dumpfile=emp_data.dmp directory=DATA_PUMP_DIR tables=(emp) content=data_only logfile=emp_data.log
 
10) Export table with metadata only:

       Below command for taking single table with metadata only.
 
       expdp scott/tiger dumpfile=emp_metadata.sql directory=DATA_PUMP_DIR tables=(emp) content=metadata_only logfile=emp_metadata.log
 
11) Export Full database without logfile:

      expdp system/manager dumpfile=full.dmp directory=DATA_PUMP_DIR full=y nologfile=y

Estimate parameter: 

This parameter will tell us how much space a new export job is going to consume. the space estimation is always in terms of bytes. we can specify the database to provide us with estimates using either 
number of database blocks or optimizer statistics. 

12) Export with parameter estimate (blocks and statistics) 
 
       expdp scott/tiger dumpfile=scott_estimate.dmp directory=data_pump_dir  logfile=scott_estimate.log estimate=blocks

       expdp scott/tiger dumpfile=scott_estimate.dmp directory=data_pump_dir  logfile=scott_estimate.log  estimate=statistics
 
INCLUDE and EXCLUDE parameters:

The INCLUDE and EXCLUDE parameters can be used to limit the export/import to specific objects. When the INCLUDE parameter is used, only those objects specified by it will be included in the export. When the EXCLUDE parameter is used, all objects except those specified by it will be included in the export. The two parameters are mutually exclusive, so use the parameter that requires the least entries to give you the result you require. The basic syntax for both parameters is the same.
 
13) Export with EXCLUDE and INCLUDE examples:

       
       expdp system/manager dumpfile=scott_1.dmp directory=data_pump_dir logfile=scott_1.log schemas=scott exclude=table:"in('EMP')"
  
-excluding database objetcs with table EMP in SCOTT schema.
  expdp system/manager dumpfile=scott_2.dmp directory=data_pump_dir logfile=scott_2.log schemas=scott exclude=procedure,trigger,function,sequence,index,table:"in('EMP')"
  
-excluding table which starts with T under SCOTT schema.
 
 expdp scott/tiger directory=data_pump_dir dumpfile=scott_schema.dmp logfile=scott_3.log schemas=scott exclude=table:"like'T%'"
  
-including table which starts with S under SCOTT schema.
 
 expdp scott/tiger directory=data_pump_dir dumpfile=scott_schema1.dmp logfile=scott_4.log schemas=scott include=table:"like'S%'"
 expdp scott/tiger schemas=SCOTT include=TABLE:"IN ('EMP', 'DEPT')" directory=TEST_DIR dumpfile=SCOTT.dmp logfile=expdpSCOTT.log
  
14) Export with QUERY Option: Predicate clause used to export a subset of a table.

     with using QUERY option
 
 expdp scott/tiger QUERY=emp:'"WHERE deptno = 10 AND sal > 10000"' DIRECTORY=data_pump_dir DUMPFILE=exp1.dmp logfile=scott_5.log
   
Go through with my previous articles related to Datapump:

impdp - ORA-31640 ORA-31693 ORA-19505 ORA-27037
ORA-31685 Error while import using IMPDP
Script: Shell Script to export Full DB or Single or Multiple Schemas Export Logical Database Backup Dumps to ASM Disk

--impdp - ORA-29913: error in executing ODCIEXTTABLEOPEN callout   solution :  remove or reduce the parallel value 

-Here you can find commands for expdp and impdp for VIEW's.. 

expdp system/manager schemas=Schema_name directory=DATA_PUMP_DIR dumpfile=all_views.dmp include=view 

impdp system/manager schemas=Schema_name directory=DATA_PUMP_DIR dumpfile=all_views.dmp 


expdp source_schema@source_schema_instance dumpfile=dump_file_name.dmp logfile=expdp_log_file_name.log tables=table_name_or_list directory=DATA_PUMP_DIR FLASHBACK_TIME="TO_TIMESTAMP('01-01-2015 14:35:00', 'DD-MM-YYYY HH24:MI:SS')"
impdp target_schema@target_schema_instance dumpfile=dump_file_name.dmp logfile=impdp_log_file_name.log tables=table_name_or_list directory=DATA_PUMP_DIR TABLE_EXISTS_ACTION=APPEND PARTITION_OPTIONS=MERGE

impdp admin2/market TABLES=customers,sales DIRECTORY=dpump1  NETWORK_LINK=chicago

impdp test/test@db10g tables=SCOTT.EMP network_link=REMOTE_SCOTT directory=TEST_DIR logfile=impdpSCOTT.log remap_schema=SCOTT:TEST


11g:
 COMPRESSION = ALL-METADATA_ONLY-NONE-DATA_ONLY