Rman partial restore on a temporary server:
-------------------------------------------

1) Add the name of the primary database (AUXNAME) into $ORATAB file
2) Ask the storage team the TDPO configuration for the primary database
3) copy the spfile in the cfg directory for the AUXNAME database
3'') Adapt the target database’s parameter file (admin/cfg/init.ora) to convert automatically the files (db_file_name_convert and log_file_name_convert).
4) startup nomount the AUXNAME database
5) Create the rman duplicate command file with the following content:

Create the duplicate.rman script:
run {
SET  UNTIL SCN --SCN;
allocate auxiliary channel TDPO0 type 'SBT_TAPE'  parms 'ENV=(TDPO_OPTFILE=/APPL/TIVOLI/TSM/dsmconfig/PRIMARYDATABASE.tdpo.opt)';
duplicate target database to AUXNAME nofilenamecheck TABLESPACE 'SYSTEM','UNDO','SYSAUX','AUDITING','MARTINS','MARTINSI','OWBSYS';
}


DO NOT SKIP TABLESPACES having Materialized Views or other known restrictions.
As documented, tablespaces to be skipped in duplicate must be self-contained.
It is a restriction that you can not SKIP TABLEPSACES containing MATERIALIZED Views 

-- To identify the tbs to restore that contain mviews
SELECT DISTINCT tablespace_name
  FROM dba_segments
 WHERE (owner, segment_name) IN (SELECT owner, mview_name FROM dba_mviews);


6) rman target sys@AGIDWA catalog RMAN11_P@catalog auxiliary / cmdfile duplicate.rman log=duplicate.log

7) drop the database when it is useless.

------------------------------------------------------------------------------------

Example:
run {
set until time "to_date('24-JAN-2011:16:15:00', 'DD-MON-YYYY:HH24:MI:SS')" ;

SET NEWNAME FOR DATABASE TO '/u01/app/oracle/oradata/FCNDUP/%b';
SET NEWNAME FOR TEMPFILE 1 TO '/u01/app/oracle/oradata/FCNDUP/tempfile01.dbf' ;

DUPLICATE TARGET DATABASE
TO FCNDUP
SKIP TABLESPACE
'EXAMPLE',           # This Tablespace contains "MATERIALIZED VIEWS"
'RCVCAT',
'DATA'
LOGFILE
GROUP 1 ('/u01/app/oracle/oradata/FCNDUP/redo01a.log',
'/u01/app/oracle/oradata/FCNDUP/redo01b.log') SIZE 4M REUSE,
GROUP 2 ('/u01/app/oracle/oradata/FCNDUP/redo02a.log',
'/u01/app/oracle/oradata/FCNDUP/redo02b.log') SIZE 4M REUSE,
GROUP 3 ('/u01/app/oracle/oradata/FCNDUP/redo03a.log',
'/u01/app/oracle/oradata/FCNDUP/redo03b.log') SIZE 4M REUSE ;
}




-------------------------

			RMAN Create Historical Subset Data

Table data is deleted from a production database oracp3. Flashback query no longer returns data:

SQL> select * from INSIDE_ILORACLE.IL_FAQ as of timestamp sysdate - 0.9;
 create table temp as select * from INSIDE_ILORACLE.IL_FAQ as of timestamp sysdate - 0.9
                                                    * 
 ERROR at line 1: 
 ORA-01555: snapshot too old: rollback segment number 51 with name "_SYSSMU51$" too small

Need to create a subset auxiliary database to restore data. The following is mostly for 10g. Anything 
specific to 11g will be indicated.



On production (dcprpcora2b):

mkdir /u01/temp/davidh
cd /u01/temp/davidh
mkdir audit archive bdump udump cdump

vi initdavidh.ora
	(you can also create it in <db ORACLE_HOME>/dbs)
	(db_file_name_convert and log_file_name_convert assumes /u01/temp/davidh can store all the datafiles needed.
	 If not, choose some other filesystem. See Note 1 below.)
*.audit_file_dest='/u01/temp/davidh/audit'
*.background_dump_dest='/u01/temp/davidh/bdump'
#*.compatible='10.2.0.3.0' #this param is probably not needed
*.control_files='/u01/temp/davidh/control.ctl'
*.core_dump_dest='/u01/temp/davidh/cdump'
*.db_block_size=8192
*.db_cache_advice='OFF'
*.db_domain='mdacc.tmc.edu'
*.db_name='davidh'
*.dispatchers=''
*.processes=100
*.log_archive_dest_1='location=/u01/temp/davidh/archive'
*.remote_login_passwordfile='exclusive'
#use small SGA
*.sga_target=300000000
*.undo_management='AUTO'
*.user_dump_dest='/u01/temp/davidh/udump'
#Don't forget tempfile location:
db_file_name_convert=('+DATA/oracp3/datafile','/u01/temp/davidh','+DATA/oracp3/tempfile','/u01/temp/davidh')
#Logfile conversion is probably not needed for the duplicate command:
log_file_name_convert=('+DATA/oracp3/datafile','/u01/temp/davidh','+FRA/oracp3/onlinelog','/u01/temp/davidh')
#See Note:334899.1
_no_recovery_through_resetlogs=TRUE

Add davidh to /etc/oratab

. oraenv
davidh

Create password file:
cd <db ORACLE_HOME>/dbs
orapwd file=orapwdavidh password=xxxxxxxx


/* Ignore the following (only needed if you have trouble with rman connecting to oracp32 through SQL*Net)
Add to <db ORACLE_HOME>/network/admin/tnsnames.ora:

davidh =
  (DESCRIPTION =
    (ADDRESS_LIST =
      (ADDRESS = (PROTOCOL = TCP)(HOST = dcprpcora2b)(PORT = 1521))
    )
    (CONNECT_DATA =
      (SID = davidh)
    )
  )

Modify <ASM ORACLE_HOME>/network/admin/listener.ora to have the SID_DESC for davidh 

SID_LIST_LISTENER_DCPRPCORA2B =
  (SID_LIST =
    (SID_DESC =
      (SID_NAME = PLSExtProc)
      (ORACLE_HOME = /u01/app/oracle/product/10.2.0/asm)
      (PROGRAM = extproc)
    )
    (SID_DESC =
      (GLOBAL_DBNAME = davidh)
      (ORACLE_HOME = /u01/app/oracle/product/10.2.0/db)
      (SID_NAME = davidh)
    )
  )

<ASM ORACLE_HOME>/bin/lsnrctl reload LISTENER_DCPRPCORA2B
(If it's SID_LIST_LISTENER in listener.ora, the command should be
<ASM ORACLE_HOME>/bin/lsnrctl reload LISTENER)


Make sure sqlplus can connect to sys through SQL*Net:
sqlplus sys/xxxxxxxx@davidh as sysdba
End ignore */


sqlplus / as sysdba
startup nomount pfile=/u01/temp/davidh/initdavidh.ora
May not need: If NFS is used (see Note 1): alter system set events '10298 trace name context forever, level 32';


/* Ignore
Make sure we're in ORACLE_HOME of target DB i.e. oracp32:
. oraenv
oracp32
End ignore */

May not need:
If NFS is used (see Note 1):
sqlplus sys/oracp3syspassword@oracp32 as sysdba
alter system set events '10298 trace name context forever, level 32';

rman target sys/oracp3syspassword@oracp32 catalog rman@oirp2 auxiliary /

10g syntax:
duplicate target database to davidh
pfile=/u01/temp/davidh/initdavidh.ora
skip tablespace 
'APEP_DATA','APEP_INDEX','AUDIT_DATA','BOXI2_DATA','CT_DATA','CT_INDEX','EBP_DATA',
'EBP_INDEX','INSIDE3_ISD1','MYMDA','NPC_DATA','NPC_INDEX','PSFT_DATA','QC_DATA',
'RXMASTER','SABA_DATA','TRACS_DATA','TRACS_INDEX','TSS_DATA','TSS_INDEX','USERS'
until time "to_date('20090309 08:00','yyyymmdd hh24:mi')"
logfile '/u01/temp/davidh/redo01.log' size 10m,
        '/u01/temp/davidh/redo02.log' size 10m,
        '/u01/temp/davidh/redo03.log' size 10m;

Make sure SYSTEM, UNDO, SYSAUX, TEMP and the needed tablespace, in our case ISDC which contains the messed up table, 
are NOT skipped.

If you get ORA-05589 or RMAN-05589 (Materialized objects in skipped tablespaces prevent duplication), add the 
tablespaces mentioned in the error message.

In 11g, you can also specify the tablespace(s) you need (no need to specify SYSTEM, UNDO, SYSAUX, which are included
automatically), plus all tablespaces with materialized views (see Note 7 below):

11g syntax:
duplicate target database to davidh
pfile=/u01/temp/davidh/initdavidh.ora
tablespace 'BATCH_DATA'
until time "to_date('20090309 08:00','yyyymmdd hh24:mi')"
logfile '/u01/temp/davidh/redo01.log' size 10m,
        '/u01/temp/davidh/redo02.log' size 10m,
        '/u01/temp/davidh/redo03.log' size 10m;

After duplicate finishes, open davidh by `alter database open resetlogs'. (11g automatically does this.)


Duplicate may throw all kinds of errors. They're documented below in the Note section, newer ones toward the end of the list.


Note:

1. Estimate storage needed by adding all datafiles for these tablespaces: system, sysaux, undo*, and the one you need, and 
add all archive logs that start from the restore starting point (which is usually an incremental0 or full backup) to the restore 
end point. If you get ORA-05589 or RMAN-05589 (see above), add the sizes of the tablespaces indicated in the error.

If an NFS mount is needed, follow this instruction (take ddrrpcora1a as an example for NFS server).

On NFS server ddrrpcora1a:
vi /etc/exports		#add or uncomment /ext/export *(rw,async) to the file
exportfs -a		#provide NFS mount
showmount		#should show no NFS client connected yet
mkdir -p /ext/export/tmp
chmod 777 /ext/export/tmp
service splx stop	#speed up I/O

On client (dcprpcora2a):

mkdir -p /ext/export/tmp
mount -t nfs -o hard,intr,rsize=32768,wsize=32768 ddrrpcora1a:/ext/export/tmp /ext/export/tmp
	#intr so you can kill a hanging cmd if NFS server hangs, rsize and wsize to speed up I/O
service splx stop	#don't waste CPU

Since /ext/export/tmp is used to store datafiles, the parameters db_file_name_convert and log_file_name_convert should 
have that instead of /u01/temp/davidh. Paths in other params can stay the same or can change to /ext/export/tmp, too.


2. Since oracp3 is RAC and davidh is not, the duplicate command may throw an error:

contents of Memory Script:
{
   Alter clone database open resetlogs;
}
executing Memory Script

RMAN-00571: ===========================================================
RMAN-00569: =============== ERROR MESSAGE STACK FOLLOWS ===============
RMAN-00571: ===========================================================
RMAN-03002: failure of Duplicate Db command at 03/10/2009 15:34:46
RMAN-03015: error occurred in stored script Memory Script
RMAN-06136: ORACLE error from auxiliary database: ORA-38856: cannot mark instance UNNAMED_INSTANCE_2 (redo thread 2) as enabled

The solution is to add _no_recovery_through_resetlogs=TRUE according to Note:334899.1 and restart the auxiliary and open resetlogs.

(It's possible we could have avoided this by having instance_number=1 in initdavidh.ora without setting the undocumented param. Untested.)


3. Note:335851.1 (Automatic TSPITR in 10G RMAN -A walk Through) should NOT be followed because the command

recover tablespace "TEST" until time "23-SEP-2005 10:00:00','DD-MON-YYYY HH24:MI:SS"
      auxiliary destination 'D:\BAckup\temp';

run in prod will revert the live production tablespace to the old state, not creating another auxiliary database and staying there for you to explore.


4. The last phase of duplicate tries to drop skipped tablespaces. It may throw errors:

executing Memory Script

database opened

contents of Memory Script:
{
# drop offline and skipped tablespaces
sql clone "drop tablespace  USERS including contents cascade constraints";
# drop offline and skipped tablespaces
sql clone "drop tablespace  TSS_INDEX including contents cascade constraints";
...
sql statement: drop tablespace  USERS including contents cascade constraints
RMAN-00571: ===========================================================
RMAN-00569: =============== ERROR MESSAGE STACK FOLLOWS ===============
RMAN-00571: ===========================================================
RMAN-03002: failure of Duplicate Db command at 06/10/2010 10:56:29
RMAN-03015: error occurred in stored script Memory Script
RMAN-03009: failure of sql command on clone_default channel at 06/10/2010 10:56:29
RMAN-11003: failure during parse/execution of SQL statement: drop tablespace  USERS including contents cascade constraints
ORA-12919: Can not drop the default permanent tablespace

It's because USERS is the database default tablespace (select * from database_properties where property_name='DEFAULT_PERMANENT_TABLESPACE').
You can safely ignore it. Alternatively, we can temporarily change the production DB default tablespace to either system or the restored 
tablespace before doing all the work:
alter database default tablespace system;
and change it back to users after the work is done.


5. If duplicate fails because only a few archive logs are not found:

RMAN-06054: media recovery requesting unknown log: thread 1 seq 49184 lowscn 65583552185

see if the archivelogs were deleted:

select distinct sequence# from v$backup_archivelog_details where thread# = 1 and sequence# between 49180 and 49190 order by 1;
select * from v$archived_log where thread# = 1 and sequence# between 49180 and 49190; --focus on DELETED column

Check other threads (thread 2 e.g.) as well.

If there's a standby where the archivelogs still exist, logon standby:

$ rman target / #do NOT specify catalog
run {
allocate channel t device type disk format '/tmp/arc_%u';  <-- assume /tmp has enough space
backup archivelog from sequence 49183 until sequence 49185 thread 1;
}

scp the files to primary (dcprpcora2a), where you run
$ rman target / catalog ...
catalog start with '<dir path>/arc';


6. If you see this error for duplicate:
	RMAN-03014: implicit resync of recovery catalog failed
	RMAN-03009: failure of partial resync command on clone_default channel at 03/27/2012 15:25:16
just retry.


7. If you see ORA-05589 or RMAN-05589, add the tablespaces that RMAN says contain materialized views to the list for the duplicate 
command and try again. Re-check file system free space if needed. If a huge tablespace must be added due to a few unimportant mviews, 
see if they can be dropped, or moved: alter table <mview's container name, check dba_mviews) move tablespace <tablespace you need to 
restore anyway>. Indexes associated with the mview (technically, its container table) may need to be rebuilt into the tablespace too. 
Grant quota as needed.


8. If the auxiliary instance wants to create change tracking file in the same path as the target database and so can't, work around by 
temporarily disable it in target: alter database disable block change tracking.


9. If duplicate looks for a very old archive log whose timestamp is older than your level 0 backup you used for the restore, there're 
two possibilities.

* Bug 12625205 or 8554110: The root cause (my paraphrase) is that if you specify "until time...", and Oracle's buggy algorithm to 
convert time to log sequence fails to find the log, it automatically assumes a very old log, which normally no longer exists. The 
workaround is to check v$log_history and find the log sequence yourself and use it in "until sequence...", or in some cases, use SCN.

* If you already ran duplicate earlier (either just a few minutes ago or a few months ago), leaving restored datafiles in the path 
you're using this time, duplicate by default has the optimization to not restore it again. If the previously restored datafile required 
a very old log (either due to the bug said above or because the datafile was there a few months ago genuinely needing the old log back 
then), your new duplicate will keep asking for the old log, even if you move "until time..." to very recent. The solution is to append 
"noresume" to the end of the duplicate command. Ref: 1549172.1
